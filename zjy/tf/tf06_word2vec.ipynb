{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec without eager\n",
    "\n",
    "### word embedding\n",
    "\n",
    "将源数据映射到另外一个空间，\n",
    "单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中。\n",
    "\n",
    "给出一个文档，文档就是一个单词序列比如 “A B A C B F G”, 希望对文档中每个不同的单词都得到一个对应的向量(往往是低维向量)表示。\n",
    "比如，对于这样的“A B A C B F G”的一个序列，也许我们最后能得到：A对应的向量为[0.1 0.6 -0.5]，B对应的向量为[-0.2 0.9 0.7] （此处的数值只用于示意）\n",
    "之所以希望把每个单词变成一个向量，目的还是为了方便计算，比如“求单词A的同义词”，就可以通过“求与单词A在cos距离下最相似的向量”来做到。\n",
    "\n",
    "比较简单的方法：\n",
    "基于BOW的one-hot；\n",
    "缺点：没有相邻单词的信息，向量可能会非常的长\n",
    "\n",
    "#### 共现矩阵Cocurrence matrix：\n",
    "\n",
    "一个非常重要的思想是，我们认为某个词的意思跟它临近的单词是紧密相关的。这是我们可以设定一个窗口（大小一般是5~10），如下窗口大小是2，那么在这个窗口内，与rests 共同出现的单词就有life、he、in、peace。然后我们就利用这种共现关系来生成词向量。\n",
    "<img src=\"https://img-blog.csdn.net/20170904134137027\" style=\"width:450px;height:100px\">\n",
    "例如，现在我们的语料库包括下面三份文档资料：\n",
    "I like deep learning. \n",
    "I like NLP. \n",
    "I enjoy flying.\n",
    "作为示例，我们设定的窗口大小为1，也就是只看某个单词周围紧邻着的那个单词。此时，将得到一个对称矩阵——共现矩阵。因为在我们的语料库中，I 和 like做为邻居同时出现在窗口中的次数是2，所以下表中I 和like相交的位置其值就是2。这样我们也实现了将word变成向量的设想，在共现矩阵每一行（或每一列）都是对应单词的一个向量表示。\n",
    "\n",
    "<img src=\"https://img-blog.csdn.net/20170904134757679\" style=\"width:400px;height:300px\">\n",
    "\n",
    "为了缩小向量，会采用SVD或PCA等降维方法。但是SVD操作计算量巨大(此处不赘述，以后有空补充)\n",
    "\n",
    "而深度学习流行之后，Tomas Mikolov 提出了Word2vec，其中涉及了两种基于neural network的方法：CBOW 和 Skip-Gram。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" starter code for word2vec skip-gram model with NCE loss\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 04\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector #版本更新，embedding投影工具\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import word2vec_utils   #这两个文件在example文件夹下，别忘了加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard 的一个内置的可视化工具 Embedding Projector, 是个交互式的可视化，可用来分析诸如 embeddings 的高维数据。 \n",
    "\n",
    "\n",
    "embedding projector 将从你的 checkpoint 文件中读取 embeddings。 \n",
    "\n",
    "\n",
    "默认情况下，embedding projector 会用 PCA 主成分分析方法将高维数据投影到 3D 空间, 还有一种投影方法是 T-SNE。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1             # the context window\n",
    "NUM_SAMPLED = 64            # number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "VISUAL_FLD = 'visualization'\n",
    "SKIP_STEP = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "NUM_VISUALIZE = 3000        # number of tokens to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(dataset):\n",
    "    \"\"\" Build the graph for word2vec model and train it \"\"\"\n",
    "    # Step 1: get input, output from the dataset\n",
    "    with tf.name_scope('data'):\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        center_words, target_words = iterator.get_next()\n",
    "\n",
    "    \"\"\" Step 2 + 3: define weights and embedding lookup.\n",
    "    In word2vec, it's actually the weights that we care about \n",
    "    \"\"\"\n",
    "    with tf.name_scope('embed'):\n",
    "        embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                        initializer=tf.random_uniform_initializer())\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n",
    "\n",
    "    # Step 4: construct variables for NCE loss and define loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n",
    "        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                            biases=nce_bias, \n",
    "                                            labels=target_words, \n",
    "                                            inputs=embed, \n",
    "                                            num_sampled=NUM_SAMPLED, \n",
    "                                            num_classes=VOCAB_SIZE), name='loss')\n",
    "\n",
    "    # Step 5: define optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    utils.safe_mkdir('checkpoints')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
    "\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            try:\n",
    "                loss_batch, _ = sess.run([loss, optimizer])\n",
    "                total_loss += loss_batch\n",
    "                if (index + 1) % SKIP_STEP == 0:\n",
    "                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                    total_loss = 0.0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n",
    "                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/text8.zip already exists\n",
      "Average loss at step 4999:  65.2\n",
      "Average loss at step 9999:  18.4\n",
      "Average loss at step 14999:   9.7\n",
      "Average loss at step 19999:   6.7\n",
      "Average loss at step 24999:   5.7\n",
      "Average loss at step 29999:   5.2\n",
      "Average loss at step 34999:   5.0\n",
      "Average loss at step 39999:   4.8\n",
      "Average loss at step 44999:   4.8\n",
      "Average loss at step 49999:   4.8\n",
      "Average loss at step 54999:   4.7\n",
      "Average loss at step 59999:   4.7\n",
      "Average loss at step 64999:   4.6\n",
      "Average loss at step 69999:   4.7\n",
      "Average loss at step 74999:   4.6\n",
      "Average loss at step 79999:   4.6\n",
      "Average loss at step 84999:   4.7\n",
      "Average loss at step 89999:   4.7\n",
      "Average loss at step 94999:   4.6\n",
      "Average loss at step 99999:   4.6\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n",
    "    word2vec(dataset)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
