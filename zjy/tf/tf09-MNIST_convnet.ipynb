{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MNIST中运用CNN\n",
    "\n",
    "(未使用tf.layer，比较麻烦的定义方式，但是便于理解)\n",
    "\n",
    "用两个卷积层，每层附带一个relu层和一个最大池化层，最后两个全连接层。卷积层的步幅是[1,1,1,1]\n",
    "\n",
    "<img src=\"./picture/mnist_convnet.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Using convolutional net on MNIST dataset of handwritten digits\n",
    "MNIST dataset: http://yann.lecun.com/exdb/mnist/\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 07\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer\n",
    "\n",
    "卷积层和ReLU层通常放在一起定义，以下先定义一个通用的卷积层函数\n",
    "\n",
    "#### tensorflow中的卷积操作算子：  \n",
    "tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=True, data_format='NHWC', dilations=[1, 1, 1, 1],          name=None)  \n",
    "各参数含义：  \n",
    "Input : Batch size(N) * Height(H) * Width(W) * Channels(C)  \n",
    "Filter: Height * Width * Input Channels * Output Channels (e.g. [5, 5, 3, 64])  \n",
    "Strides: 4 element 1-D tensor, strides in each direction (often [1, 1, 1, 1] or [1, 2, 2, 1])  \n",
    "Padding:  A string from: \"SAME\", \"VALID\"  \n",
    "Dilations(膨胀系数？？): Defaults to [1, 1, 1, 1]. 1-D tensor of length 4. The dilation factor for each dimension of input. If set to k > 1, there will be k-1 skipped cells between each filter element on that dimension.    Dilations in the batch and depth dimensions must be 1.  \n",
    "Data_format: default to NHWC  \n",
    "\n",
    "\n",
    "tf.get_variable():  \n",
    "如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_channels = inputs.shape[-1]\n",
    "        kernel = tf.get_variable('kernel', [k_size, k_size, in_channels, filters],\n",
    "                                initializer=tf.random_normal_initializer())\n",
    "        biases = tf.get_variable('biases', [filters], initializer=tf.random_normal_initializer())\n",
    "        conv = tf.nn.conv2d(inputs, kernel,strides=[1,stride, stride, 1], padding= padding)\n",
    "    return tf.nn.relu(conv + biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很重要的一点是我们要一直明确我们的输出规模，规模的定义公式：\n",
    "\t\t $$(W-F + 2P) / S + 1$$\n",
    "其中：\n",
    "    the input volume size (W)  \n",
    "    the receptive field size of filter (F)  \n",
    "    the stride with which they are applied (S)  \n",
    "    the amount of zero padding used (P) on the border  \n",
    "例：输入为$7*7$，filter为$3*3$，步幅为1，填充为0，则输出为$5*5$($（7-3+2*0）/1+1 = 5$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer 池化层\n",
    "\n",
    "池化层是一个下采样过程，作用是减少维度并尽可能保证特征。最常用的池化方式是the max pooling。\n",
    "\n",
    "在tensorflow中，有专门的池化算子tf.nn.max_pool，同样的先创建池化层函数\n",
    "\n",
    "tf.nn.max_pool(\n",
    "    value,\n",
    "    ksize,\n",
    "    strides,\n",
    "    padding,\n",
    "    data_format='NHWC',\n",
    "    name=None\n",
    ")  \n",
    "Args:  \n",
    "value: A 4-D Tensor of the format specified by data_format.   \n",
    "ksize: A 1-D int Tensor of 4 elements. The size of the window for each dimension of the input tensor.  \n",
    "strides: A 1-D int Tensor of 4 elements. The stride of the sliding window for each dimension of the input tensor.  \n",
    "padding: A string, either 'VALID' or 'SAME'. The padding algorithm. See the comment here  \n",
    "data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.  \n",
    "name: Optional name for the operation.  \n",
    "Returns:  \n",
    "A Tensor of format specified by data_format. The max pooled output tensor.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(inputs,\n",
    "                             ksize=[1, ksize, ksize, 1], \n",
    "                             strides=[1, ksize, ksize,1], \n",
    "                             padding=padding)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，我们关注输出的规模，公式如下：\n",
    "\t\t$$(W-K + 2P) / S + 1$$\n",
    "其中：  \n",
    "the input volume size (W)  \n",
    "the pooling size (K)  \n",
    "the stride with which they are applied (S)  \n",
    "the amount of zero padding used (P) on the border  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer\n",
    "就。最常见的全连接层。  \n",
    "\n",
    "fc = tf.matmul(pool2, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_dim = inputs.shape[-1]\n",
    "        w = tf.get_variable('weights', [in_dim, out_dim], \n",
    "                           initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [out_dim],\n",
    "                           initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.matmul(inputs, w) + b\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "tensor中的数值以keep_prob的概率保留。其他置0。  \n",
    "tf.nn.dropout(  \n",
    "    x,  \n",
    "    keep_prob,  \n",
    "    noise_shape=None,  \n",
    "    seed=None,  \n",
    "    name=None  \n",
    ")  \n",
    "Args:  \n",
    "x: A floating point tensor.  \n",
    "keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.  \n",
    "noise_shape: A 1-D Tensor of type int32, representing the shape for randomly generated keep/drop flags.  \n",
    "seed: A Python integer. Used to create random seeds. See tf.set_random_seed for behavior.  \n",
    "name: A name for this operation (optional).  \n",
    "Returns:  \n",
    "A Tensor of the same shape of x.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "    #Step 1 : set \n",
    "    def _init_(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32,\n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 10000\n",
    "        self.training = True\n",
    "    \n",
    "    #Step 2 : input data\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            #from tensorflow.examples.tutorials.mnist import input_data\n",
    "            #mnist = input_data.read_data_sets('/MNIST_DATA', one_hot = True)\n",
    "            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n",
    "            \n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                      train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "             # reshape the image to make it work with tf.nn.conv2d\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            \n",
    "             # initializer for train_data\n",
    "            self.train_init = iterator.make_initializer(train_data) \n",
    "             # initializer for test_data\n",
    "            self.test_init = iterator.make_initializer(test_data) \n",
    "            \n",
    "    #Step 3 : build the model\n",
    "    def inference(self):\n",
    "        conv1 = conv_relu(inputs=self.img, \n",
    "                         filters=32, \n",
    "                         k_size = 5, \n",
    "                         stride=1, \n",
    "                         padding='SAME', \n",
    "                         scope_name='conv1')\n",
    "        pool1 = maxpool(inputs=conv1,\n",
    "                        ksize=2, \n",
    "                        stride=2, \n",
    "                        padding='VALID', \n",
    "                        scope_name='pool')\n",
    "        conv2 = conv_relu(inputs=pool1,\n",
    "                        filters=64,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv2')\n",
    "        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n",
    "        #将pool2展开，以便fc层计算\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        \n",
    "        fc = fully_connected(inputs=pool2, \n",
    "                            out_dim=1024, \n",
    "                            scope_name='fc')\n",
    "        #为防止过拟合，做一次dropout\n",
    "        dropout = tf.nn.dropout(x=tf.nn.relu(fc), \n",
    "                               keep_prob= self.keep_prob, \n",
    "                                name='relu_dropout')\n",
    "        #第二层fc\n",
    "        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n",
    "        \n",
    "    #Step 4 : define the loss function\n",
    "    #define loss function\n",
    "    #use softmax cross entropy with logits as the loss function\n",
    "    #compute mean cross entropy, softmax is applied internally\n",
    "    def loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, \n",
    "                                                             logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "            \n",
    "    #Step 5 : difine optimizer\n",
    "    #using Adam Gradient Descent to minimize cost\n",
    "    def optimize(self):\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "        \n",
    "            \n",
    "    #Step 6 : 求accuracy\n",
    "    #Count the number of right predictions in a batch\n",
    "    def eval(self):\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "            \n",
    "    #Step 7 : visualize with tensorboard\n",
    "    #Create summaries to write on TensorBoard\n",
    "    def summary(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "        \n",
    "    ###组合啦\n",
    "    def build(self):\n",
    "        self._init_()\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    " \n",
    "\n",
    "    #训练开始！\n",
    "    #先定义一次训练\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step = step)\n",
    "                #间隔几步，输出loss\n",
    "                if(step + 1) % self.skip_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += 1\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "             pass\n",
    "        saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "    \n",
    "    #单次的accuracy\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = False\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step = step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "            \n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "    \n",
    "    #The train function alternates between training one epoch and evaluating\n",
    "    def train(self, n_epochs):\n",
    "        utils.safe_mkdir('checkpoints')\n",
    "        utils.safe_mkdir('checkpoints/convnet_mnist')\n",
    "        writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "            \n",
    "            #如果已有数据，则读取以保存的数据\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "            step = self.gstep.eval()\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-b04afdd2a18d>:73: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Loss at step 19: 22725.66796875\n",
      "Loss at step 39: 10053.849609375\n",
      "Loss at step 59: 6540.755859375\n",
      "Loss at step 79: 5954.5361328125\n",
      "Loss at step 99: 3885.522705078125\n",
      "Loss at step 119: 3472.58837890625\n",
      "Loss at step 139: 2871.009521484375\n",
      "Loss at step 159: 1993.1129150390625\n",
      "Loss at step 179: 2132.438232421875\n",
      "Loss at step 199: 2661.772216796875\n",
      "Loss at step 219: 2204.09619140625\n",
      "Loss at step 239: 1850.6143798828125\n",
      "Loss at step 259: 1916.8681640625\n",
      "Loss at step 279: 663.19287109375\n",
      "Loss at step 299: 1371.8922119140625\n",
      "Loss at step 319: 917.9982299804688\n",
      "Loss at step 339: 762.3031005859375\n",
      "Loss at step 359: 1035.0814208984375\n",
      "Loss at step 379: 758.4645385742188\n",
      "Loss at step 399: 1118.3355712890625\n",
      "Loss at step 419: 790.3795166015625\n",
      "Average loss at epoch 0: 1.0\n",
      "Took: 99.80129194259644 seconds\n",
      "Accuracy at epoch 0: 0.9027 \n",
      "Took: 6.428572416305542 seconds\n",
      "Loss at step 439: 822.2576293945312\n",
      "Loss at step 459: 793.4281005859375\n",
      "Loss at step 479: 1396.06689453125\n",
      "Loss at step 499: 456.26300048828125\n",
      "Loss at step 519: 710.605224609375\n",
      "Loss at step 539: 404.1192626953125\n",
      "Loss at step 559: 185.23556518554688\n",
      "Loss at step 579: 513.4483642578125\n",
      "Loss at step 599: 784.1248168945312\n",
      "Loss at step 619: 716.6053466796875\n",
      "Loss at step 639: 569.4902954101562\n",
      "Loss at step 659: 636.105224609375\n",
      "Loss at step 679: 915.7074584960938\n",
      "Loss at step 699: 542.1187744140625\n",
      "Loss at step 719: 469.2893371582031\n",
      "Loss at step 739: 866.4873046875\n",
      "Loss at step 759: 24.17144775390625\n",
      "Loss at step 779: 463.093994140625\n",
      "Loss at step 799: 531.9816284179688\n",
      "Loss at step 819: 367.4853210449219\n",
      "Loss at step 839: 259.7848205566406\n",
      "Loss at step 859: 401.2373352050781\n",
      "Average loss at epoch 1: 1.0\n",
      "Took: 95.2460126876831 seconds\n",
      "Accuracy at epoch 1: 0.9404 \n",
      "Took: 6.132705211639404 seconds\n",
      "Loss at step 879: 611.9263916015625\n",
      "Loss at step 899: 332.15191650390625\n",
      "Loss at step 919: 247.58702087402344\n",
      "Loss at step 939: 476.53656005859375\n",
      "Loss at step 959: 592.6343994140625\n",
      "Loss at step 979: 466.5561218261719\n",
      "Loss at step 999: 221.83863830566406\n",
      "Loss at step 1019: 368.26617431640625\n",
      "Loss at step 1039: 428.3193359375\n",
      "Loss at step 1059: 217.42050170898438\n",
      "Loss at step 1079: 141.7003631591797\n",
      "Loss at step 1099: 149.93338012695312\n",
      "Loss at step 1119: 167.77371215820312\n",
      "Loss at step 1139: 31.20655059814453\n",
      "Loss at step 1159: 242.28326416015625\n",
      "Loss at step 1179: 236.3658905029297\n",
      "Loss at step 1199: 351.2344055175781\n",
      "Loss at step 1219: 241.2433319091797\n",
      "Loss at step 1239: 167.04624938964844\n",
      "Loss at step 1259: 127.53590393066406\n",
      "Loss at step 1279: 114.6064224243164\n",
      "Average loss at epoch 2: 1.0\n",
      "Took: 98.57176542282104 seconds\n",
      "Accuracy at epoch 2: 0.9483 \n",
      "Took: 6.2784717082977295 seconds\n",
      "Loss at step 1299: 79.6903076171875\n",
      "Loss at step 1319: 161.4794921875\n",
      "Loss at step 1339: 43.160003662109375\n",
      "Loss at step 1359: 161.7361602783203\n",
      "Loss at step 1379: 261.13153076171875\n",
      "Loss at step 1399: 334.91107177734375\n",
      "Loss at step 1419: 71.74085998535156\n",
      "Loss at step 1439: 85.92351531982422\n",
      "Loss at step 1459: 118.84255981445312\n",
      "Loss at step 1479: 67.02985382080078\n",
      "Loss at step 1499: 101.9012451171875\n",
      "Loss at step 1519: 138.5088348388672\n",
      "Loss at step 1539: 110.41193389892578\n",
      "Loss at step 1559: 201.7784881591797\n",
      "Loss at step 1579: 205.7279052734375\n",
      "Loss at step 1599: 120.15130615234375\n",
      "Loss at step 1619: 145.73248291015625\n",
      "Loss at step 1639: 96.39691162109375\n",
      "Loss at step 1659: 111.77197265625\n",
      "Loss at step 1679: 191.53457641601562\n",
      "Loss at step 1699: 69.96908569335938\n",
      "Loss at step 1719: 197.25750732421875\n",
      "Average loss at epoch 3: 1.0\n",
      "Took: 103.58895254135132 seconds\n",
      "Accuracy at epoch 3: 0.9543 \n",
      "Took: 6.111351728439331 seconds\n",
      "Loss at step 1739: 362.182861328125\n",
      "Loss at step 1759: 82.59762573242188\n",
      "Loss at step 1779: 155.11978149414062\n",
      "Loss at step 1799: 122.95333099365234\n",
      "Loss at step 1819: 180.70343017578125\n",
      "Loss at step 1839: 114.09838104248047\n",
      "Loss at step 1859: 51.49005126953125\n",
      "Loss at step 1879: 130.9328155517578\n",
      "Loss at step 1899: 150.3331756591797\n",
      "Loss at step 1919: 95.17866516113281\n",
      "Loss at step 1939: 118.6565933227539\n",
      "Loss at step 1959: 100.75016021728516\n",
      "Loss at step 1979: 225.94149780273438\n",
      "Loss at step 1999: 83.8404541015625\n",
      "Loss at step 2019: 137.56689453125\n",
      "Loss at step 2039: 126.88156127929688\n",
      "Loss at step 2059: 97.91271209716797\n",
      "Loss at step 2079: 212.93341064453125\n",
      "Loss at step 2099: 190.57412719726562\n",
      "Loss at step 2119: 197.10049438476562\n",
      "Loss at step 2139: 240.47740173339844\n",
      "Average loss at epoch 4: 1.0\n",
      "Took: 110.6030502319336 seconds\n",
      "Accuracy at epoch 4: 0.955 \n",
      "Took: 6.803344249725342 seconds\n",
      "Loss at step 2159: 1.6302413940429688\n",
      "Loss at step 2179: 82.85502624511719\n",
      "Loss at step 2199: 175.7392578125\n",
      "Loss at step 2219: 57.86528778076172\n",
      "Loss at step 2239: 36.39305114746094\n",
      "Loss at step 2259: 88.364013671875\n",
      "Loss at step 2279: 0.0\n",
      "Loss at step 2299: 170.12222290039062\n",
      "Loss at step 2319: 53.3451042175293\n",
      "Loss at step 2339: 97.55461883544922\n",
      "Loss at step 2359: 21.300445556640625\n",
      "Loss at step 2379: 63.43424987792969\n",
      "Loss at step 2399: 23.49649429321289\n",
      "Loss at step 2419: 148.78720092773438\n",
      "Loss at step 2439: 24.494354248046875\n",
      "Loss at step 2459: 105.11097717285156\n",
      "Loss at step 2479: 153.61587524414062\n",
      "Loss at step 2499: 42.946685791015625\n",
      "Loss at step 2519: 59.577484130859375\n",
      "Loss at step 2539: 152.44183349609375\n",
      "Loss at step 2559: 54.162620544433594\n",
      "Loss at step 2579: 1.7220348119735718\n",
      "Average loss at epoch 5: 1.0\n",
      "Took: 99.63963580131531 seconds\n",
      "Accuracy at epoch 5: 0.9589 \n",
      "Took: 5.613997459411621 seconds\n",
      "Loss at step 2599: 186.8930206298828\n",
      "Loss at step 2619: 37.79448699951172\n",
      "Loss at step 2639: 45.80315017700195\n",
      "Loss at step 2659: 15.128299713134766\n",
      "Loss at step 2679: 65.04301452636719\n",
      "Loss at step 2699: 45.50092315673828\n",
      "Loss at step 2719: 216.8228302001953\n",
      "Loss at step 2739: 80.37783813476562\n",
      "Loss at step 2759: 201.112060546875\n",
      "Loss at step 2779: 37.77737045288086\n",
      "Loss at step 2799: 0.78607177734375\n",
      "Loss at step 2819: 97.654541015625\n",
      "Loss at step 2839: 38.29838562011719\n",
      "Loss at step 2859: 17.192777633666992\n",
      "Loss at step 2879: 110.54774475097656\n",
      "Loss at step 2899: 7.507148742675781\n",
      "Loss at step 2919: 45.64459228515625\n",
      "Loss at step 2939: 54.401817321777344\n",
      "Loss at step 2959: 53.867061614990234\n",
      "Loss at step 2979: 73.90322875976562\n",
      "Loss at step 2999: 13.322748184204102\n",
      "Average loss at epoch 6: 1.0\n",
      "Took: 93.1333155632019 seconds\n",
      "Accuracy at epoch 6: 0.9601 \n",
      "Took: 5.609994173049927 seconds\n",
      "Loss at step 3019: 85.04643249511719\n",
      "Loss at step 3039: 30.497047424316406\n",
      "Loss at step 3059: 23.653587341308594\n",
      "Loss at step 3079: 40.392051696777344\n",
      "Loss at step 3099: 42.783878326416016\n",
      "Loss at step 3119: 23.66564178466797\n",
      "Loss at step 3139: 8.047370910644531\n",
      "Loss at step 3159: 112.55062103271484\n",
      "Loss at step 3179: 17.28258514404297\n",
      "Loss at step 3199: 30.350440979003906\n",
      "Loss at step 3219: 100.74561309814453\n",
      "Loss at step 3239: 38.77200698852539\n",
      "Loss at step 3259: 38.134979248046875\n",
      "Loss at step 3279: 83.51560974121094\n",
      "Loss at step 3299: 7.2244873046875\n",
      "Loss at step 3319: 102.60820007324219\n",
      "Loss at step 3339: 48.440101623535156\n",
      "Loss at step 3359: 80.279052734375\n",
      "Loss at step 3379: 11.587284088134766\n",
      "Loss at step 3399: 47.61281204223633\n",
      "Loss at step 3419: 47.76701354980469\n",
      "Loss at step 3439: 0.0\n",
      "Average loss at epoch 7: 1.0\n",
      "Took: 92.85380530357361 seconds\n",
      "Accuracy at epoch 7: 0.9644 \n",
      "Took: 5.60799241065979 seconds\n",
      "Loss at step 3459: 7.2695465087890625\n",
      "Loss at step 3479: 59.50822448730469\n",
      "Loss at step 3499: 68.10325622558594\n",
      "Loss at step 3519: 42.39505386352539\n",
      "Loss at step 3539: 90.03128814697266\n",
      "Loss at step 3559: 13.53341293334961\n",
      "Loss at step 3579: 35.625572204589844\n",
      "Loss at step 3599: 34.01691436767578\n",
      "Loss at step 3619: 17.514427185058594\n",
      "Loss at step 3639: 31.95014190673828\n",
      "Loss at step 3659: 10.85858154296875\n",
      "Loss at step 3679: 32.559688568115234\n",
      "Loss at step 3699: 45.927005767822266\n",
      "Loss at step 3719: 24.404129028320312\n",
      "Loss at step 3739: 57.85503387451172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 3759: 110.52007293701172\n",
      "Loss at step 3779: 22.517333984375\n",
      "Loss at step 3799: 56.43974304199219\n",
      "Loss at step 3819: 84.72364807128906\n",
      "Loss at step 3839: 79.27716064453125\n",
      "Loss at step 3859: 2.6389198303222656\n",
      "Average loss at epoch 8: 1.0\n",
      "Took: 92.86406707763672 seconds\n",
      "Accuracy at epoch 8: 0.9634 \n",
      "Took: 5.631995916366577 seconds\n",
      "Loss at step 3879: 8.198116302490234\n",
      "Loss at step 3899: 16.56704330444336\n",
      "Loss at step 3919: 27.444305419921875\n",
      "Loss at step 3939: 38.54176712036133\n",
      "Loss at step 3959: 16.897443771362305\n",
      "Loss at step 3979: 6.151445388793945\n",
      "Loss at step 3999: 3.4584617614746094\n",
      "Loss at step 4019: 94.47312927246094\n",
      "Loss at step 4039: 26.588287353515625\n",
      "Loss at step 4059: 0.0\n",
      "Loss at step 4079: 28.846712112426758\n",
      "Loss at step 4099: 33.18427658081055\n",
      "Loss at step 4119: 26.835031509399414\n",
      "Loss at step 4139: 26.467370986938477\n",
      "Loss at step 4159: 8.624481201171875\n",
      "Loss at step 4179: 42.34376525878906\n",
      "Loss at step 4199: 10.636581420898438\n",
      "Loss at step 4219: 51.45255661010742\n",
      "Loss at step 4239: 18.34949493408203\n",
      "Loss at step 4259: 2.0786514282226562\n",
      "Loss at step 4279: 7.153596878051758\n",
      "Loss at step 4299: 9.567349433898926\n",
      "Average loss at epoch 9: 1.0\n",
      "Took: 93.18525099754333 seconds\n",
      "Accuracy at epoch 9: 0.9673 \n",
      "Took: 5.620001554489136 seconds\n",
      "Loss at step 4319: 1.2918167114257812\n",
      "Loss at step 4339: 6.758754730224609\n",
      "Loss at step 4359: 19.646804809570312\n",
      "Loss at step 4379: 37.1694450378418\n",
      "Loss at step 4399: 35.45977020263672\n",
      "Loss at step 4419: 62.549190521240234\n",
      "Loss at step 4439: 30.053573608398438\n",
      "Loss at step 4459: 131.738037109375\n",
      "Loss at step 4479: 19.27707290649414\n",
      "Loss at step 4499: 56.842063903808594\n",
      "Loss at step 4519: 46.532691955566406\n",
      "Loss at step 4539: 25.938629150390625\n",
      "Loss at step 4559: 29.4945068359375\n",
      "Loss at step 4579: 74.1302719116211\n",
      "Loss at step 4599: 28.931304931640625\n",
      "Loss at step 4619: 12.261894226074219\n",
      "Loss at step 4639: 68.41758728027344\n",
      "Loss at step 4659: 1.5361480712890625\n",
      "Loss at step 4679: 13.50033950805664\n",
      "Loss at step 4699: 68.72538757324219\n",
      "Loss at step 4719: 13.220069885253906\n",
      "Average loss at epoch 10: 1.0\n",
      "Took: 93.41035914421082 seconds\n",
      "Accuracy at epoch 10: 0.9702 \n",
      "Took: 5.602989435195923 seconds\n",
      "Loss at step 4739: 31.504362106323242\n",
      "Loss at step 4759: 0.0\n",
      "Loss at step 4779: 91.84201049804688\n",
      "Loss at step 4799: 29.448379516601562\n",
      "Loss at step 4819: 13.464838027954102\n",
      "Loss at step 4839: 85.98040008544922\n",
      "Loss at step 4859: 16.18242645263672\n",
      "Loss at step 4879: 0.8714828491210938\n",
      "Loss at step 4899: 53.44587326049805\n",
      "Loss at step 4919: 4.650890350341797\n",
      "Loss at step 4939: 61.050514221191406\n",
      "Loss at step 4959: 21.532146453857422\n",
      "Loss at step 4979: 29.228519439697266\n",
      "Loss at step 4999: 31.957931518554688\n",
      "Loss at step 5019: 40.6358642578125\n",
      "Loss at step 5039: 0.0\n",
      "Loss at step 5059: 47.218379974365234\n",
      "Loss at step 5079: 26.933162689208984\n",
      "Loss at step 5099: 19.81806182861328\n",
      "Loss at step 5119: 107.11043548583984\n",
      "Loss at step 5139: 26.371742248535156\n",
      "Loss at step 5159: 22.835716247558594\n",
      "Average loss at epoch 11: 1.0\n",
      "Took: 93.0075466632843 seconds\n",
      "Accuracy at epoch 11: 0.9727 \n",
      "Took: 5.613996267318726 seconds\n",
      "Loss at step 5179: 28.814056396484375\n",
      "Loss at step 5199: 77.1230697631836\n",
      "Loss at step 5219: 16.6341552734375\n",
      "Loss at step 5239: 45.80959701538086\n",
      "Loss at step 5259: 29.080482482910156\n",
      "Loss at step 5279: 0.0\n",
      "Loss at step 5299: 20.564727783203125\n",
      "Loss at step 5319: 0.0\n",
      "Loss at step 5339: 23.171119689941406\n",
      "Loss at step 5359: 7.881932258605957\n",
      "Loss at step 5379: 0.0\n",
      "Loss at step 5399: 32.97233581542969\n",
      "Loss at step 5419: 63.03827667236328\n",
      "Loss at step 5439: 66.82978057861328\n",
      "Loss at step 5459: 29.563453674316406\n",
      "Loss at step 5479: 1.50244140625\n",
      "Loss at step 5499: 67.164794921875\n",
      "Loss at step 5519: 10.949928283691406\n",
      "Loss at step 5539: 52.74164581298828\n",
      "Loss at step 5559: 0.24541473388671875\n",
      "Loss at step 5579: 30.635272979736328\n",
      "Average loss at epoch 12: 1.0\n",
      "Took: 93.0841760635376 seconds\n",
      "Accuracy at epoch 12: 0.9698 \n",
      "Took: 5.602989912033081 seconds\n",
      "Loss at step 5599: 15.106819152832031\n",
      "Loss at step 5619: 20.283605575561523\n",
      "Loss at step 5639: 0.0\n",
      "Loss at step 5659: 41.38594436645508\n",
      "Loss at step 5679: 69.48741149902344\n",
      "Loss at step 5699: 24.378612518310547\n",
      "Loss at step 5719: 13.907777786254883\n",
      "Loss at step 5739: 41.18242263793945\n",
      "Loss at step 5759: 13.387527465820312\n",
      "Loss at step 5779: 12.76910400390625\n",
      "Loss at step 5799: 54.46767044067383\n",
      "Loss at step 5819: 103.08879089355469\n",
      "Loss at step 5839: 2.0559921264648438\n",
      "Loss at step 5859: 16.918434143066406\n",
      "Loss at step 5879: 7.894161224365234\n",
      "Loss at step 5899: 21.452129364013672\n",
      "Loss at step 5919: 45.80718994140625\n",
      "Loss at step 5939: 5.182838439941406\n",
      "Loss at step 5959: 12.452621459960938\n",
      "Loss at step 5979: 29.84564208984375\n",
      "Loss at step 5999: 1.0030860900878906\n",
      "Loss at step 6019: 0.5250244140625\n",
      "Average loss at epoch 13: 1.0\n",
      "Took: 93.03703284263611 seconds\n",
      "Accuracy at epoch 13: 0.9686 \n",
      "Took: 5.65802788734436 seconds\n",
      "Loss at step 6039: 35.33665084838867\n",
      "Loss at step 6059: 1.72100830078125\n",
      "Loss at step 6079: 32.277896881103516\n",
      "Loss at step 6099: 12.25766372680664\n",
      "Loss at step 6119: 0.0\n",
      "Loss at step 6139: 33.03212356567383\n",
      "Loss at step 6159: 21.894256591796875\n",
      "Loss at step 6179: 44.27456283569336\n",
      "Loss at step 6199: 13.357139587402344\n",
      "Loss at step 6219: 23.078144073486328\n",
      "Loss at step 6239: 120.4378662109375\n",
      "Loss at step 6259: 2.4915390014648438\n",
      "Loss at step 6279: 2.6371994018554688\n",
      "Loss at step 6299: 18.44107437133789\n",
      "Loss at step 6319: 0.318267822265625\n",
      "Loss at step 6339: 0.0\n",
      "Loss at step 6359: 13.4503173828125\n",
      "Loss at step 6379: 0.0\n",
      "Loss at step 6399: 0.0\n",
      "Loss at step 6419: 14.556716918945312\n",
      "Loss at step 6439: 105.07038879394531\n",
      "Average loss at epoch 14: 1.0\n",
      "Took: 92.65954208374023 seconds\n",
      "Accuracy at epoch 14: 0.9744 \n",
      "Took: 5.591963052749634 seconds\n",
      "Loss at step 6459: 1.1216468811035156\n",
      "Loss at step 6479: 6.8548736572265625\n",
      "Loss at step 6499: 8.518302917480469\n",
      "Loss at step 6519: 25.123538970947266\n",
      "Loss at step 6539: 5.702816009521484\n",
      "Loss at step 6559: 0.0\n",
      "Loss at step 6579: 3.7189407348632812\n",
      "Loss at step 6599: 19.468345642089844\n",
      "Loss at step 6619: 28.906349182128906\n",
      "Loss at step 6639: 21.849197387695312\n",
      "Loss at step 6659: 14.176984786987305\n",
      "Loss at step 6679: 37.32876205444336\n",
      "Loss at step 6699: 4.638282775878906\n",
      "Loss at step 6719: 0.07935745269060135\n",
      "Loss at step 6739: 0.0\n",
      "Loss at step 6759: 11.515544891357422\n",
      "Loss at step 6779: 29.225379943847656\n",
      "Loss at step 6799: 3.0211410522460938\n",
      "Loss at step 6819: 22.628307342529297\n",
      "Loss at step 6839: 0.0\n",
      "Loss at step 6859: 5.048107147216797\n",
      "Loss at step 6879: 3.977006435394287\n",
      "Average loss at epoch 15: 1.0\n",
      "Took: 92.59964656829834 seconds\n",
      "Accuracy at epoch 15: 0.9719 \n",
      "Took: 5.593982219696045 seconds\n",
      "Loss at step 6899: 59.559059143066406\n",
      "Loss at step 6919: 27.555702209472656\n",
      "Loss at step 6939: 6.076454162597656\n",
      "Loss at step 6959: 2.8451995849609375\n",
      "Loss at step 6979: 0.0\n",
      "Loss at step 6999: 23.337032318115234\n",
      "Loss at step 7019: 6.297399520874023\n",
      "Loss at step 7039: 13.579559326171875\n",
      "Loss at step 7059: 0.0\n",
      "Loss at step 7079: 17.09307098388672\n",
      "Loss at step 7099: 41.57680892944336\n",
      "Loss at step 7119: 61.60064697265625\n",
      "Loss at step 7139: 25.226566314697266\n",
      "Loss at step 7159: 70.08560943603516\n",
      "Loss at step 7179: 9.205619812011719\n",
      "Loss at step 7199: 0.0\n",
      "Loss at step 7219: 53.34138488769531\n",
      "Loss at step 7239: 6.428138732910156\n",
      "Loss at step 7259: 0.0\n",
      "Loss at step 7279: 11.157021522521973\n",
      "Loss at step 7299: 0.0\n",
      "Average loss at epoch 16: 1.0\n",
      "Took: 92.73869204521179 seconds\n",
      "Accuracy at epoch 16: 0.9758 \n",
      "Took: 5.634011268615723 seconds\n",
      "Loss at step 7319: 0.0\n",
      "Loss at step 7339: 0.0\n",
      "Loss at step 7359: 8.875320434570312\n",
      "Loss at step 7379: 0.0\n",
      "Loss at step 7399: 28.321178436279297\n",
      "Loss at step 7419: 0.0\n",
      "Loss at step 7439: 15.144378662109375\n",
      "Loss at step 7459: 0.0\n",
      "Loss at step 7479: 0.0\n",
      "Loss at step 7499: 11.28350830078125\n",
      "Loss at step 7519: 18.762378692626953\n",
      "Loss at step 7539: 50.89409255981445\n",
      "Loss at step 7559: 5.260221481323242\n",
      "Loss at step 7579: 0.0\n",
      "Loss at step 7599: 5.589660759142134e-06\n",
      "Loss at step 7619: 20.73676300048828\n",
      "Loss at step 7639: 1.9683685302734375\n",
      "Loss at step 7659: 17.522354125976562\n",
      "Loss at step 7679: 27.13010597229004\n",
      "Loss at step 7699: 23.289772033691406\n",
      "Loss at step 7719: 8.373025894165039\n",
      "Loss at step 7739: 0.4401910901069641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at epoch 17: 1.0\n",
      "Took: 92.82866907119751 seconds\n",
      "Accuracy at epoch 17: 0.9745 \n",
      "Took: 5.597985029220581 seconds\n",
      "Loss at step 7759: 8.407346725463867\n",
      "Loss at step 7779: 0.0\n",
      "Loss at step 7799: 17.826385498046875\n",
      "Loss at step 7819: 45.348236083984375\n",
      "Loss at step 7839: 3.0579490661621094\n",
      "Loss at step 7859: 0.0\n",
      "Loss at step 7879: 38.969600677490234\n",
      "Loss at step 7899: 31.420196533203125\n",
      "Loss at step 7919: 0.0\n",
      "Loss at step 7939: 19.144752502441406\n",
      "Loss at step 7959: 17.492549896240234\n",
      "Loss at step 7979: 0.0\n",
      "Loss at step 7999: 8.784873962402344\n",
      "Loss at step 8019: 0.0\n",
      "Loss at step 8039: 33.79181671142578\n",
      "Loss at step 8059: 0.0\n",
      "Loss at step 8079: 0.0\n",
      "Loss at step 8099: 11.764053344726562\n",
      "Loss at step 8119: 3.9808349609375\n",
      "Loss at step 8139: 0.0\n",
      "Loss at step 8159: 14.672096252441406\n",
      "Average loss at epoch 18: 1.0\n",
      "Took: 92.50471258163452 seconds\n",
      "Accuracy at epoch 18: 0.9746 \n",
      "Took: 5.606974124908447 seconds\n",
      "Loss at step 8179: 46.80978775024414\n",
      "Loss at step 8199: 20.297801971435547\n",
      "Loss at step 8219: 0.0\n",
      "Loss at step 8239: 7.975955963134766\n",
      "Loss at step 8259: 2.5406856536865234\n",
      "Loss at step 8279: 5.726325988769531\n",
      "Loss at step 8299: 23.802330017089844\n",
      "Loss at step 8319: 0.0\n",
      "Loss at step 8339: 0.0\n",
      "Loss at step 8359: 8.351966857910156\n",
      "Loss at step 8379: 39.355430603027344\n",
      "Loss at step 8399: 4.551303863525391\n",
      "Loss at step 8419: 6.734495162963867\n",
      "Loss at step 8439: 25.080081939697266\n",
      "Loss at step 8459: 0.0\n",
      "Loss at step 8479: 7.980777740478516\n",
      "Loss at step 8499: 9.928508758544922\n",
      "Loss at step 8519: 1.9210052490234375\n",
      "Loss at step 8539: 32.189876556396484\n",
      "Loss at step 8559: 7.332130432128906\n",
      "Loss at step 8579: 0.0\n",
      "Loss at step 8599: 35.83831787109375\n",
      "Average loss at epoch 19: 1.0\n",
      "Took: 92.49027991294861 seconds\n",
      "Accuracy at epoch 19: 0.9755 \n",
      "Took: 5.586978197097778 seconds\n",
      "Loss at step 8619: 3.124286651611328\n",
      "Loss at step 8639: 34.96019744873047\n",
      "Loss at step 8659: 0.0\n",
      "Loss at step 8679: 50.87107849121094\n",
      "Loss at step 8699: 3.00079345703125\n",
      "Loss at step 8719: 0.0\n",
      "Loss at step 8739: 3.347423553466797\n",
      "Loss at step 8759: 7.217994689941406\n",
      "Loss at step 8779: 4.7268524169921875\n",
      "Loss at step 8799: 12.676856994628906\n",
      "Loss at step 8819: 43.48833084106445\n",
      "Loss at step 8839: 14.3072509765625\n",
      "Loss at step 8859: 0.0\n",
      "Loss at step 8879: 31.161785125732422\n",
      "Loss at step 8899: 13.045490264892578\n",
      "Loss at step 8919: 14.35940933227539\n",
      "Loss at step 8939: 0.0\n",
      "Loss at step 8959: 0.0\n",
      "Loss at step 8979: 0.0\n",
      "Loss at step 8999: 0.0\n",
      "Loss at step 9019: 39.87367248535156\n",
      "Average loss at epoch 20: 1.0\n",
      "Took: 92.9042763710022 seconds\n",
      "Accuracy at epoch 20: 0.9774 \n",
      "Took: 5.5979859828948975 seconds\n",
      "Loss at step 9039: 73.21979522705078\n",
      "Loss at step 9059: 6.586032867431641\n",
      "Loss at step 9079: 0.0\n",
      "Loss at step 9099: 74.10430145263672\n",
      "Loss at step 9119: 37.581356048583984\n",
      "Loss at step 9139: 18.512371063232422\n",
      "Loss at step 9159: 0.0\n",
      "Loss at step 9179: 1.3096084594726562\n",
      "Loss at step 9199: 12.285591125488281\n",
      "Loss at step 9219: 16.883556365966797\n",
      "Loss at step 9239: 4.381416320800781\n",
      "Loss at step 9259: 0.0\n",
      "Loss at step 9279: 2.8108444213867188\n",
      "Loss at step 9299: 0.0\n",
      "Loss at step 9319: 0.0\n",
      "Loss at step 9339: 66.74364471435547\n",
      "Loss at step 9359: 17.67302703857422\n",
      "Loss at step 9379: 21.793712615966797\n",
      "Loss at step 9399: 0.0\n",
      "Loss at step 9419: 13.665409088134766\n",
      "Loss at step 9439: 0.0\n",
      "Loss at step 9459: 41.9316520690918\n",
      "Average loss at epoch 21: 1.0\n",
      "Took: 92.61410093307495 seconds\n",
      "Accuracy at epoch 21: 0.9773 \n",
      "Took: 5.587979555130005 seconds\n",
      "Loss at step 9479: 7.021453857421875\n",
      "Loss at step 9499: 2.0519332885742188\n",
      "Loss at step 9519: 0.0\n",
      "Loss at step 9539: 21.741561889648438\n",
      "Loss at step 9559: 6.561958312988281\n",
      "Loss at step 9579: 6.637355804443359\n",
      "Loss at step 9599: 8.167488098144531\n",
      "Loss at step 9619: 14.284980773925781\n",
      "Loss at step 9639: 0.0\n",
      "Loss at step 9659: 30.52215576171875\n",
      "Loss at step 9679: 0.0\n",
      "Loss at step 9699: 0.0\n",
      "Loss at step 9719: 6.602794647216797\n",
      "Loss at step 9739: 8.256553649902344\n",
      "Loss at step 9759: 45.772403717041016\n",
      "Loss at step 9779: 11.210399627685547\n",
      "Loss at step 9799: 0.0\n",
      "Loss at step 9819: 7.858879089355469\n",
      "Loss at step 9839: 4.4608612060546875\n",
      "Loss at step 9859: 0.0\n",
      "Loss at step 9879: 18.632080078125\n",
      "Average loss at epoch 22: 1.0\n",
      "Took: 92.78650712966919 seconds\n",
      "Accuracy at epoch 22: 0.9785 \n",
      "Took: 5.590980529785156 seconds\n",
      "Loss at step 9899: 0.0\n",
      "Loss at step 9919: 23.236587524414062\n",
      "Loss at step 9939: 0.0\n",
      "Loss at step 9959: 17.291309356689453\n",
      "Loss at step 9979: 23.288963317871094\n",
      "Loss at step 9999: 0.0\n",
      "Loss at step 10019: 0.0\n",
      "Loss at step 10039: 0.0\n",
      "Loss at step 10059: 1.8978805541992188\n",
      "Loss at step 10079: 36.624656677246094\n",
      "Loss at step 10099: 9.966297149658203\n",
      "Loss at step 10119: 0.0\n",
      "Loss at step 10139: 12.774303436279297\n",
      "Loss at step 10159: 0.0\n",
      "Loss at step 10179: 12.767826080322266\n",
      "Loss at step 10199: 6.963844299316406\n",
      "Loss at step 10219: 0.0\n",
      "Loss at step 10239: 0.0\n",
      "Loss at step 10259: 0.0\n",
      "Loss at step 10279: 23.325958251953125\n",
      "Loss at step 10299: 0.0\n",
      "Loss at step 10319: 0.0\n",
      "Average loss at epoch 23: 1.0\n",
      "Took: 92.66518974304199 seconds\n",
      "Accuracy at epoch 23: 0.9759 \n",
      "Took: 5.6149985790252686 seconds\n",
      "Loss at step 10339: 0.0\n",
      "Loss at step 10359: 1.8580093383789062\n",
      "Loss at step 10379: 5.277698516845703\n",
      "Loss at step 10399: 0.0\n",
      "Loss at step 10419: 0.0\n",
      "Loss at step 10439: 19.636474609375\n",
      "Loss at step 10459: 0.0\n",
      "Loss at step 10479: 0.0005126399919390678\n",
      "Loss at step 10499: 0.0\n",
      "Loss at step 10519: 23.9771728515625\n",
      "Loss at step 10539: 0.28443145751953125\n",
      "Loss at step 10559: 33.006500244140625\n",
      "Loss at step 10579: 0.70513916015625\n",
      "Loss at step 10599: 6.69403076171875\n",
      "Loss at step 10619: 2.6235809326171875\n",
      "Loss at step 10639: 9.506448745727539\n",
      "Loss at step 10659: 1.8299179077148438\n",
      "Loss at step 10679: 0.0\n",
      "Loss at step 10699: 0.0\n",
      "Loss at step 10719: 10.673324584960938\n",
      "Loss at step 10739: 17.67028045654297\n",
      "Average loss at epoch 24: 1.0\n",
      "Took: 92.84011244773865 seconds\n",
      "Accuracy at epoch 24: 0.9769 \n",
      "Took: 5.625005006790161 seconds\n",
      "Loss at step 10759: 0.0\n",
      "Loss at step 10779: 6.719581604003906\n",
      "Loss at step 10799: 13.860280990600586\n",
      "Loss at step 10819: 4.882469177246094\n",
      "Loss at step 10839: 25.192516326904297\n",
      "Loss at step 10859: 0.0\n",
      "Loss at step 10879: 10.838615417480469\n",
      "Loss at step 10899: 0.4297523498535156\n",
      "Loss at step 10919: 0.0\n",
      "Loss at step 10939: 0.0\n",
      "Loss at step 10959: 11.812652587890625\n",
      "Loss at step 10979: 0.0\n",
      "Loss at step 10999: 20.132183074951172\n",
      "Loss at step 11019: 0.03701784461736679\n",
      "Loss at step 11039: 0.0\n",
      "Loss at step 11059: 0.0\n",
      "Loss at step 11079: 13.194843292236328\n",
      "Loss at step 11099: 0.0\n",
      "Loss at step 11119: 0.0\n",
      "Loss at step 11139: 8.472217559814453\n",
      "Loss at step 11159: 0.0\n",
      "Loss at step 11179: 0.0\n",
      "Average loss at epoch 25: 1.0\n",
      "Took: 92.86870574951172 seconds\n",
      "Accuracy at epoch 25: 0.9797 \n",
      "Took: 5.621002435684204 seconds\n",
      "Loss at step 11199: 4.2493438720703125\n",
      "Loss at step 11219: 0.0\n",
      "Loss at step 11239: 0.0\n",
      "Loss at step 11259: 0.0\n",
      "Loss at step 11279: 0.0\n",
      "Loss at step 11299: 0.0\n",
      "Loss at step 11319: 0.0\n",
      "Loss at step 11339: 0.0\n",
      "Loss at step 11359: 4.776294708251953\n",
      "Loss at step 11379: 3.178173065185547\n",
      "Loss at step 11399: 2.6531448364257812\n",
      "Loss at step 11419: 1.209075927734375\n",
      "Loss at step 11439: 0.0\n",
      "Loss at step 11459: 0.0\n",
      "Loss at step 11479: 0.0\n",
      "Loss at step 11499: 19.765357971191406\n",
      "Loss at step 11519: 29.11193084716797\n",
      "Loss at step 11539: 10.397506713867188\n",
      "Loss at step 11559: 14.368171691894531\n",
      "Loss at step 11579: 0.0\n",
      "Loss at step 11599: 2.278278350830078\n",
      "Average loss at epoch 26: 1.0\n",
      "Took: 92.5120918750763 seconds\n",
      "Accuracy at epoch 26: 0.9799 \n",
      "Took: 5.587978839874268 seconds\n",
      "Loss at step 11619: 0.0\n",
      "Loss at step 11639: 18.884429931640625\n",
      "Loss at step 11659: 0.07837139070034027\n",
      "Loss at step 11679: 26.03887939453125\n",
      "Loss at step 11699: 0.3334617614746094\n",
      "Loss at step 11719: 0.0\n",
      "Loss at step 11739: 0.0\n",
      "Loss at step 11759: 12.030838012695312\n",
      "Loss at step 11779: 8.826484680175781\n",
      "Loss at step 11799: 0.0\n",
      "Loss at step 11819: 21.278972625732422\n",
      "Loss at step 11839: 18.33544158935547\n",
      "Loss at step 11859: 0.0\n",
      "Loss at step 11879: 3.7869186401367188\n",
      "Loss at step 11899: 0.0\n",
      "Loss at step 11919: 6.1480255126953125\n",
      "Loss at step 11939: 21.631912231445312\n",
      "Loss at step 11959: 4.0667877197265625\n",
      "Loss at step 11979: 0.0\n",
      "Loss at step 11999: 0.0\n",
      "Loss at step 12019: 11.293720245361328\n",
      "Loss at step 12039: 9.613714218139648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at epoch 27: 1.0\n",
      "Took: 92.59288167953491 seconds\n",
      "Accuracy at epoch 27: 0.9813 \n",
      "Took: 5.600987911224365 seconds\n",
      "Loss at step 12059: 0.0\n",
      "Loss at step 12079: 0.0\n",
      "Loss at step 12099: 0.0\n",
      "Loss at step 12119: 7.424171447753906\n",
      "Loss at step 12139: 1.0775260925292969\n",
      "Loss at step 12159: 3.5080795288085938\n",
      "Loss at step 12179: 6.86175537109375\n",
      "Loss at step 12199: 27.41316032409668\n",
      "Loss at step 12219: 0.0\n",
      "Loss at step 12239: 1.0388069152832031\n",
      "Loss at step 12259: 37.439544677734375\n",
      "Loss at step 12279: 30.647640228271484\n",
      "Loss at step 12299: 48.15747833251953\n",
      "Loss at step 12319: 0.0\n",
      "Loss at step 12339: 15.432842254638672\n",
      "Loss at step 12359: 0.0\n",
      "Loss at step 12379: 5.017585754394531\n",
      "Loss at step 12399: 3.95086669921875\n",
      "Loss at step 12419: 0.0\n",
      "Loss at step 12439: 0.0\n",
      "Loss at step 12459: 0.0\n",
      "Average loss at epoch 28: 1.0\n",
      "Took: 92.47008109092712 seconds\n",
      "Accuracy at epoch 28: 0.9787 \n",
      "Took: 5.592982769012451 seconds\n",
      "Loss at step 12479: 11.001678466796875\n",
      "Loss at step 12499: 0.0\n",
      "Loss at step 12519: 0.3289031982421875\n",
      "Loss at step 12539: 1.3723831176757812\n",
      "Loss at step 12559: 0.0\n",
      "Loss at step 12579: 0.0\n",
      "Loss at step 12599: 50.65359115600586\n",
      "Loss at step 12619: 28.460588455200195\n",
      "Loss at step 12639: 0.0\n",
      "Loss at step 12659: 0.0\n",
      "Loss at step 12679: 3.6530990600585938\n",
      "Loss at step 12699: 0.0\n",
      "Loss at step 12719: 0.0\n",
      "Loss at step 12739: 10.988113403320312\n",
      "Loss at step 12759: 0.0\n",
      "Loss at step 12779: 13.762584686279297\n",
      "Loss at step 12799: 1.9788131713867188\n",
      "Loss at step 12819: 18.758834838867188\n",
      "Loss at step 12839: 1.9249553680419922\n",
      "Loss at step 12859: 3.7151870727539062\n",
      "Loss at step 12879: 0.0\n",
      "Loss at step 12899: 16.903520584106445\n",
      "Average loss at epoch 29: 1.0\n",
      "Took: 95.33993148803711 seconds\n",
      "Accuracy at epoch 29: 0.981 \n",
      "Took: 6.1403727531433105 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = ConvNet()\n",
    "    model.build()\n",
    "    model.train(n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
