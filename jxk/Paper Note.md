reference

References
[1] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural Photo Editing with Introspective Adversarial Networks. In ICLR, 2017. 1, 2
[2] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversar- ial Nets. In NIPS, 2016. 2
[3] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep Gen- erative Image Models using a Laplacian Pyramid of Adver- sarial Networks. In NIPS, 2015. 1, 2
[4] J. Donahue, P. Kr¨
ahenb¨
uhl, and T. Darrell. Adversarial Fea- ture Learning. In ICLR, 2017. 2, 3 5714
[5] A. Dosovitskiy, J. Tobias Springenberg, and T. Brox. Learn- ing to Generate Chairs with Convolutional Neural Networks. In CVPR, 2015. 2
[6] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially Learned Inference. In ICLR, 2017. 2
[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.Warde-Farley, S. Ozair, A. Courville, andY. Bengio. Gen- erative Adversarial Nets. In NIPS, 2014. 1, 2
[8] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In ICML, 2015. 2
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016. 3
[10] S. Ioffe and C. Szegedy. Batch Normalization: Accelerat- ing Deep Network Training by Reducing Internal Covariate Shift. In ICML, 2015. 3
[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to- Image Translation with Conditional Adversarial Networks. In CVPR, 2017. 1, 2
[12] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses for Real-Time Style Transfer and Super-Resolution. In ECCV, 2016. 1, 2, 3
[13] D. Kingma and J. Ba. Adam: A Method for Stochastic Opti- mization. In ICLR, 2014. 5
[14] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In ICLR, 2014. 2
[15] R. Kiros, R. Salakhutdinov, and R. Zemel. Unifying Visual- Semantic Embeddings with Multimodal Neural Language Models. In TACL, 2015. 5
[16] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding Beyond Pixels Using a Learned Similarity Metric. In ICML, 2016. 2
[17] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunning- ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z.Wang, and W. Shi. Photo-Realistic Single Image Super-Resolution Us- ing a Generative Adversarial Network. In CVPR, 2017. 1, 2
[18] C. Li and M. Wand. Precomputed Real-Time Texture Syn- thesis with Markovian Generative Adversarial Networks. In ECCV, 2016. 1, 2
[19] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised Image-to-Image Translation Networks.
arXiv preprint
arXiv:1703.00848, 2017. 1, 2
[20] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial Autoencoders. In ICLR, 2016. 2
[21] M. Mirza and S. Osindero. Conditional Generative Adver- sarial Nets. arXiv preprint arXiv:1411.1784, 2014. 2
[22] M.-E. Nilsback and A. Zisserman. Automated Flower Clas- sification over a Large Number of Classes. In ICCVGIP, 2008. 2, 5
[23] A. V. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel Recurrent Neural Networks. In ICML, 2016. 2
[24] A. V. d. Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu. Conditional Image Gen- eration with PixelCNN Decoders. In NIPS, 2016. 2
[25] G. Perarnau, J.V. D.Weijer, B. Raducanu, J. M. ´ Alvarez, and
D. Csiro. Invertible Conditional GANs for Image Editing. In NIPS Workshop, 2016. 3
[26] A. Radford, L. Metz, and S. Chintala. Unsupervised Repre- sentation Learning with Deep Convolutional Generative Ad- versarial Networks. In ICLR, 2016. 1, 2
[27] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning Deep Representations of Fine-Grained Visual Descriptions. In CVPR, 2016. 2, 5
[28] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee. Learning What and Where to Draw. In NIPS, 2016. 1, 2
[29] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative Adversarial Text to Image Synthesis. In ICML, 2016. 1, 2, 3, 4, 5
[30] S. Reed, Y. Zhang, Y. Zhang, and H. Lee. Deep Visual Analogy-Making. In NIPS, 2015. 2
[31] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Gen- erative Models. In ICML, 2014. 2
[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein. Imagenet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–252, 2015. 5
[33] T. Salimans, I. Goodfellow,W. Zaremba,V. Cheung, A. Rad- ford, and X. Chen. Improved Techniques for Training GANs. In NIPS, 2016. 1, 2
[34] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2015. 5
[35] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised Cross- Domain Image Generation. In ICLR, 2017. 1, 2
[36] C.Wah, S. Branson, P.Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Re- port CNS-TR-2011-001, California Institute of Technology, 2011. 2, 5
[37] X. Wang and A. Gupta. Generative Image Modeling Using Style and Structure Adversarial Networks. In ECCV, 2016. 1, 2
[38] X.Yan, J.Yang, K. Sohn, and H. Lee. Attribute2Image: Con- ditional Image Generation from Visual Attributes. In ECCV, 2016. 2
[39] J. Yang, S. Reed, M.-H. Yang, and H. Lee. Weakly- supervised Disentangling with Recurrent Transformations for 3D View Synthesis. In NIPS, 2015. 2
[40] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixel- Level Domain Transfer. In ECCV, 2016. 1, 2
[41] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. StackGAN: Text to Photo-realistic Image Syn- thesis with Stacked Generative Adversarial Networks. arXiv preprint arXiv:1612.03242, 2016. 1, 2, 3, 8
[42] J.-Y. Zhu, P. Kr¨
ahenb¨
uhl, E. Shechtman, and A. A. Efros.
Generative Visual Manipulation on the Natural Image Mani- fold. In ECCV, 2016. 1, 2
5715





1.Introduction

Main problem: twofold?

1.model should disentangle the semantics contained in image and text description

2.combine the disentangle semantics and generate the images.

model built by GAN[7,3,26,33]

previous work which have succeed in generating images that match text descripition[29,28,41]：

New condition: given images

Proposed approaches[17,35,11,42,18,40,1,37,19,12] for various tasks.None of them involve employing natural language descriptions as conditions.

core contribution is to semantically synthesize image with text.

design an end to end neural architecture built upon GAN,conditioning on both image and text.

Training strategy based on adversarial learning.

Generator :input:original image ->encodes into feature representation->concatenates with text semantics,which encode by a pretrained encoder->decode to a synthesized image.

Discriminator:1.realistic 2.match input text descriptions.

3.Baseline method

The approach in 29 is to train a style encoder network S  to invert the generator network G, the synthesized images x was mapped back to latent variable z.

4.Method

G:encoder-decoder architecture. Encoders are employed to encode source images based on feature and texts.

D:distinguish conditioned on text semantic features.

Differences with base line method:a novel architecture with a specific loss function to optimize the learning of generator for image synthesis. Latent representation of images is from constitutional layer which contain more spartial information.]

4.1 Network architecture

Generator

Encoder is a CNN,that encodes src image of size 64 * 64 into feature representations of dimension 16 * 16 * 512.Use ReLU activation in all convolutional layers. Batch normalization is performed in all layers except first conv layer.

Text descriptions are encoded by pretarined model.Then we apply a text embedding augmentation method on it, making it a dimension of 128.Through the approach by zhang, we can get a large number of additional text embeddings, and concatenate it with the computed image feature representations.

The residual transformation unit is to ecodes the concatenated images and text feature representations jointly.

Decoder consists of serveral upsampling layers that transform the latent feature representations into synthesized 64 images.ReLU activation and batch normalization are adopted in all layers ecept the last layer.

Discriminator

First apply convolutional layers to downsample the images into feature representations  of dimension 4 4 512. Then concatenate the image representation  with text embeddings ,then apply two conv layers to produce final probabilities. leaky-ReLU activation is used for all the layers except the output layer. Batch normalization is adopted for all layers except the first layer and the last layer.

4.2 Adaptive loss for semantic image synthesis

Synthesize a realistic image that not only matches target text description but alson retains the irrelevant features. Utilize the adversarial learning to learn implicit loss functions.









